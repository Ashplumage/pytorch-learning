{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch入门第一步"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor 是高维向量，以下通过几个例子理解tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个空的tensor\n",
    "import torch as t\n",
    "x = t.Tensor(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.9184e-39, 9.0000e-39, 1.0561e-38],\n",
       "        [1.0653e-38, 4.1327e-39, 8.9082e-39],\n",
       "        [9.8265e-39, 9.4592e-39, 1.0561e-38],\n",
       "        [1.0653e-38, 1.0469e-38, 9.5510e-39],\n",
       "        [9.1837e-39, 1.0561e-38, 1.0469e-38]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1164, 0.1933, 0.2574],\n",
       "        [0.7859, 0.9826, 0.8999],\n",
       "        [0.5901, 0.3190, 0.5984],\n",
       "        [0.7702, 0.0878, 0.6523],\n",
       "        [0.0921, 0.6700, 0.8237]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 为这个空的tensor赋值\n",
    "x = t.rand(5,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "print(x.size())# 查看tensor的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "#查看行数\n",
    "print(x.size()[0])\n",
    "\n",
    "#查看列数\n",
    "print(x.size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8965, 0.3191, 0.2819],\n",
       "        [0.0410, 0.0182, 0.0821],\n",
       "        [0.3496, 0.7055, 0.0169],\n",
       "        [0.9357, 0.1216, 0.3955],\n",
       "        [0.3384, 0.7187, 0.8725]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#定义另一个tensor\n",
    "y = t.rand(5,3)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0129, 0.5125, 0.5393],\n",
       "        [0.8269, 1.0008, 0.9820],\n",
       "        [0.9397, 1.0246, 0.6153],\n",
       "        [1.7059, 0.2094, 1.0478],\n",
       "        [0.4304, 1.3887, 1.6962]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#基本加减法运算\n",
    "x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0129, 0.5125, 0.5393],\n",
       "        [0.8269, 1.0008, 0.9820],\n",
       "        [0.9397, 1.0246, 0.6153],\n",
       "        [1.7059, 0.2094, 1.0478],\n",
       "        [0.4304, 1.3887, 1.6962]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#另外一种形式的加法\n",
    "t.add(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0129, 0.5125, 0.5393],\n",
       "        [0.8269, 1.0008, 0.9820],\n",
       "        [0.9397, 1.0246, 0.6153],\n",
       "        [1.7059, 0.2094, 1.0478],\n",
       "        [0.4304, 1.3887, 1.6962]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#第三种形式的加法\n",
    "\n",
    "#预先分配好空间\n",
    "result = t.Tensor(5,3)\n",
    "t.add(x,y,out = result)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始的x: tensor([[0.1164, 0.1933, 0.2574],\n",
      "        [0.7859, 0.9826, 0.8999],\n",
      "        [0.5901, 0.3190, 0.5984],\n",
      "        [0.7702, 0.0878, 0.6523],\n",
      "        [0.0921, 0.6700, 0.8237]])\n",
      "第一次加法的x: tensor([[0.1164, 0.1933, 0.2574],\n",
      "        [0.7859, 0.9826, 0.8999],\n",
      "        [0.5901, 0.3190, 0.5984],\n",
      "        [0.7702, 0.0878, 0.6523],\n",
      "        [0.0921, 0.6700, 0.8237]])\n",
      "第二次加法的x: tensor([[1.0129, 0.5125, 0.5393],\n",
      "        [0.8269, 1.0008, 0.9820],\n",
      "        [0.9397, 1.0246, 0.6153],\n",
      "        [1.7059, 0.2094, 1.0478],\n",
      "        [0.4304, 1.3887, 1.6962]])\n"
     ]
    }
   ],
   "source": [
    "print('原始的x:',x)\n",
    "\n",
    "#还有一种是\n",
    "x.add(y)\n",
    "print('第一次加法的x:',x)\n",
    "\n",
    "#这一种的加法，也就是方法名带下划线的加法，是会将得到的值重新写入到x中，改变x原来的值\n",
    "x.add_(y)\n",
    "print('第二次加法的x:',x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2、Tensor和numpy之间的互相操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#定义一个新的tensor\n",
    "a = t.ones(5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#将a转换成numpy形式\n",
    "b = a.numpy()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n",
      "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#用numpy 定义一个新的array\n",
    "a = np.ones(5)\n",
    "print(a)\n",
    "\n",
    "#将numpy形式的array转成tensor\n",
    "b = t.from_numpy(a)\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensor 和 numpy之间共享内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原来的a: [1. 1. 1. 1. 1.]\n",
      "原来的b: tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "加1之后的a: [2. 2. 2. 2. 2.]\n",
      "加1之后的b: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#采用 _ 形式的方法，改变自身\n",
    "print('原来的a:',a)\n",
    "print('原来的b:',b)\n",
    "\n",
    "#这里仅用b改变自身，但是由于tensor 和 numpy共享内存，所以另一个由a转成b的b改变了，原来的a 也将随之改变\n",
    "b.add_(1)\n",
    "print('加1之后的a:',a)\n",
    "print('加1之后的b:',b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensor使用.cuda()形式来转成gpu计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu支持\n"
     ]
    }
   ],
   "source": [
    "if t.cuda.is_available():\n",
    "    print('本电脑支持gpu运算')\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、Autograd()自动微分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#定义一个Variable 单位矩阵\n",
    "x = Variable(t.ones(2,2),requires_grad = True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#对单位矩阵进行求和运算，得到的还是Variable形式\n",
    "y = x.sum()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SumBackward0 at 0x1c39126bcc0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#这个\n",
    "y.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#可以把y看作是一个定义在x上的函数表达式，这里对y进行反向传播梯度计算\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#然后将x的值带入进去，得到最终的梯度导数\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2.],\n",
       "        [2., 2.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#反向传播的梯度是累加的，每次在训练神经网络时候需要对梯度进行清零\n",
    "print('第二次再次计算梯度：')\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第三次再次计算梯度：\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3.],\n",
       "        [3., 3.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('第三次再次计算梯度：')\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#重新改变x.grad的值，也就是之前求的x的梯度的data\n",
    "x.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable和Tensor之间有接口，可以相互转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]], requires_grad=True)\n",
      "x_grad:\n",
      "tensor(1., grad_fn=<MeanBackward0>)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "#定义一个Variable\n",
    "x = Variable(t.ones(4,5),requires_grad = True)#这里相当于将tensor转换成Variable\n",
    "#定义一个在x上的函数\n",
    "y = t.mean(x)\n",
    "\n",
    "print(x)\n",
    "#求导\n",
    "y.backward()\n",
    "print('x_grad:')\n",
    "x.grad\n",
    "\n",
    "x_tensor_cos = t.mean(x.data)\n",
    "print(y)\n",
    "print(x_tensor_cos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4、神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#搭建一个简单的神经网络\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        \n",
    "        #定义卷积层，这里的参数和keras里面的参数不一致，1表示输入通道，6表示输出通道，5表示5*5的卷积核\n",
    "        self.conv1 = nn.Conv2d(1,6,5)\n",
    "        self.conv2 = nn.Conv2d(6,16,5)\n",
    "        \n",
    "        #定义全连接层，16*5*5，将前面的卷积层得到的图像进行拉平\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120,84)\n",
    "        self.fc3 = nn.Linear(84,10)\n",
    "        \n",
    "    def forward(self ,x):\n",
    "        #第一次计算，卷积+激活函数\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)),(2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)),2)\n",
    "        \n",
    "        #全连接层\n",
    "        x = x.view(x.size()[0],-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "#返回学习参数\n",
    "params = list(net.parameters())\n",
    "print(len(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight : torch.Size([6, 1, 5, 5])\n",
      "conv1.bias : torch.Size([6])\n",
      "conv2.weight : torch.Size([16, 6, 5, 5])\n",
      "conv2.bias : torch.Size([16])\n",
      "fc1.weight : torch.Size([120, 400])\n",
      "fc1.bias : torch.Size([120])\n",
      "fc2.weight : torch.Size([84, 120])\n",
      "fc2.bias : torch.Size([84])\n",
      "fc3.weight : torch.Size([10, 84])\n",
      "fc3.bias : torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name,parameters in net.named_parameters():\n",
    "    print(name,':',parameters.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 只有Variable有自动求导的功能，tensor需要提前封装成Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.6061,  0.2402, -0.9197,  ...,  0.6496,  0.4290,  0.0495],\n",
      "          [ 0.2581, -0.6281,  0.9362,  ..., -1.1375,  0.7761,  1.0967],\n",
      "          [-0.8418,  0.0965, -0.5410,  ...,  0.4317, -0.9814, -0.2231],\n",
      "          ...,\n",
      "          [ 0.9971, -1.4284,  0.4164,  ..., -0.9707,  0.5338,  0.6664],\n",
      "          [-0.9284,  1.2209, -0.5715,  ..., -1.0020,  0.1953,  0.1462],\n",
      "          [ 0.6149, -0.0157,  1.7840,  ...,  0.1128, -0.6761, -0.3168]]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#这个input可以看作是一张黑白的32*32的单张图像数据\n",
    "input = Variable(t.randn(1,1,32,32))\n",
    "print(input)\n",
    "\n",
    "#将数据丢入神经网络中，得到一个长度为10的数组，其中长度10表示我们之前定义的需要预测10个数字\n",
    "out = net(input)\n",
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(Variable(t.ones(1,10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测值: tensor([[-0.1325, -0.0593,  0.0214, -0.0895,  0.0529, -0.0673,  0.0426, -0.0554,\n",
      "          0.1026, -0.0714]], grad_fn=<AddmmBackward>)\n",
      "真实值: tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:443: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([1, 10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "计算得到的均方差误差为：\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(28.5785, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = net(input)\n",
    "print('预测值:',output)\n",
    "target = Variable(t.arange(0,10,1.))\n",
    "print('真实值:',target)\n",
    "\n",
    "#定义损失函数\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(output,target)\n",
    "\n",
    "print('计算得到的均方差误差为：')\n",
    "loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "反向传播之前的conv1.bais的梯度\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "反向传播之后的conv1.bais的梯度\n",
      "tensor([ 0.0605, -0.0511, -0.0184, -0.0131,  0.0552,  0.0260])\n"
     ]
    }
   ],
   "source": [
    "#将网络中的梯度清零\n",
    "net.zero_grad()\n",
    "\n",
    "#第一层卷积层的输出有6个通道，查看第一层卷积中的偏差值\n",
    "print('反向传播之前的conv1.bais的梯度')\n",
    "print(net.conv1.bias.grad)\n",
    "loss.backward()\n",
    "\n",
    "print('反向传播之后的conv1.bais的梯度')\n",
    "print(net.conv1.bias.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5、优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:443: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([1, 10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(net.parameters(),lr = 0.01)\n",
    "\n",
    "#训练过程中梯度清零\n",
    "optimizer.zero_grad()\n",
    "output = net(input)\n",
    "loss = criterion(output,target)\n",
    "\n",
    "#反向传播计算梯度，利用得到的梯度，再选择优化器，相当于人在一个山坡上，梯度相当于计算得到了这个坡度，但是还是需要人去选择怎么走\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以上便是关于pytorch入门级的一些常用的方法和怎么实现一个神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
